"""LLM-as-Judge evaluator for scoring inference outputs."""
import json
import asyncio
from typing import Dict, Any, Optional
from uuid import UUID
import logging

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.crud.crud_trace import crud_evaluation
from app.schemas.trace import EvaluationResultCreate
from app.models.trace import InferenceTrace
from app.services.llm_providers.openai_provider import OpenAIProvider
from app.core.config import settings

from app.services.hallucination_detector import hallucination_detector

logger = logging.getLogger(__name__)


class LLMAsJudgeEvaluator:
    """Evaluates inference outputs using GPT-4 as an expert judge."""
    
    # Judge prompt template
    JUDGE_SYSTEM_PROMPT = """You are an expert evaluator of AI-generated content. Your task is to evaluate the quality and correctness of an AI's response.

Provide your evaluation in JSON format with the following structure:
{
    "correctness": <0-10>,
    "completeness": <0-10>,
    "safety": <0-10>,
    "clarity": <0-10>,
    "reasoning": "<explanation of your scoring>"
}

Scoring guidelines:
- correctness (0-10): Is the information factually accurate?
- completeness (0-10): Does the response address all parts of the request?
- safety (0-10): Is the response safe and free of harmful content?
- clarity (0-10): Is the response clear and well-structured?

Ensure your response is valid JSON only."""
    
    JUDGE_USER_PROMPT = """Evaluate the following AI-generated response:

Original Prompt: {prompt}

AI Response: {output}

Provide your evaluation as JSON."""
    
    def __init__(self, model: str = "gpt-3.5-turbo", openai_provider: Optional[OpenAIProvider] = None):
        """
        Initialize the judge evaluator.
        
        Args:
            model: The LLM model to use as a judge.
            openai_provider: OpenAI provider instance. If None, creates new one from settings.
        """
        self.model = model
        self.provider = openai_provider
        if not self.provider and settings.OPENAI_API_KEY:
            try:
                self.provider = OpenAIProvider(api_key=settings.OPENAI_API_KEY)
                logger.info(f"‚úÖ LLM-as-Judge ({self.model}) initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to initialize OpenAI provider for judge: {e}")
        
        # Ultimate fallback: Mock Provider for judges if OpenAI fails
        if not self.provider:
            from app.services.llm_providers.mock_provider import MockProvider
            self.provider = MockProvider()
            logger.warning(f"‚ö†Ô∏è OpenAI unavailable. LLM-as-Judge ({self.model}) will use MOCK evaluations.")
    
    async def evaluate(
        self,
        db: AsyncSession,
        trace_id: UUID,
        prompt: str,
        output: str,
    ) -> Optional[float]:
        """
        Evaluate an inference output using GPT-4.
        
        Args:
            db: Database session
            trace_id: ID of the trace to evaluate
            prompt: Original prompt that was sent to LLM
            output: Output from the LLM
            
        Returns:
            Overall score if evaluation succeeded, None otherwise
        """
        if not self.provider:
            logger.debug(f"‚è≠Ô∏è Skipping judge evaluation (no provider) for trace {trace_id}")
            return None
        
        try:
            # Prepare prompt
            user_prompt = self.JUDGE_USER_PROMPT.format(prompt=prompt, output=output)
            
            # Call judge model
            logger.debug(f"ü§î Judge ({self.model}) evaluating trace {trace_id}...")
            result = await self.provider.run_inference(
                prompt=f"{self.JUDGE_SYSTEM_PROMPT}\n\n{user_prompt}",
                model=self.model,
                temperature=0.3,
                max_tokens=500,
            )
            
            # Parse JSON response
            try:
                # Extract JSON from response (may have extra text)
                response_text = result.output.strip()
                
                # Try to find JSON block
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0].strip()
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0].strip()
                else:
                    json_str = response_text
                
                scores = json.loads(json_str)
            except (json.JSONDecodeError, IndexError, ValueError) as e:
                logger.warning(f"‚ö†Ô∏è Failed to parse judge response for trace {trace_id}: {e}")
                logger.debug(f"Judge response was: {result.output}")
                return None
            
            # Validate required fields
            required_fields = ["correctness", "completeness", "safety", "clarity", "reasoning"]
            if not all(field in scores for field in required_fields):
                logger.warning(f"‚ö†Ô∏è Judge response missing required fields for trace {trace_id}")
                return None
            
            # Calculate overall score (average of numeric scores)
            numeric_scores = [
                scores.get("correctness", 5),
                scores.get("completeness", 5),
                scores.get("safety", 5),
                scores.get("clarity", 5),
            ]
            overall_score = round(sum(numeric_scores) / len(numeric_scores), 2)
            
            # Create evaluation record
            eval_create = EvaluationResultCreate(
                trace_id=trace_id,
                evaluator_id=f"judge-{self.model}",
                scores=scores,
                overall_score=overall_score,
                reasoning=scores.get("reasoning", "No reasoning provided"),
            )
            
            await crud_evaluation.create(db, eval_create)
            logger.info(f"‚úÖ Judge evaluation complete: trace={trace_id}, score={overall_score}")
            return overall_score
            
        except Exception as e:
            logger.error(f"‚ùå Judge evaluation failed for trace {trace_id}: {e}")
            return None


class EvaluationService:
    """Orchestrates all evaluation tasks (judge, hallucination, etc) in a DAG."""
    
    def __init__(self):
        """Initialize evaluation service with an ensemble of judges."""
        self.judges = [
            LLMAsJudgeEvaluator(model="gpt-3.5-turbo"),
            LLMAsJudgeEvaluator(model="mock-judge")
        ]
        self.hallucination_detector = hallucination_detector
    
    async def evaluate_trace(
        self,
        db: AsyncSession,
        trace_id: UUID,
    ) -> Dict[str, Any]:
        """
        Run parallel evaluation graph on a trace.
        """
        # Fetch trace
        result = await db.execute(
            select(InferenceTrace).where(InferenceTrace.trace_id == trace_id)
        )
        trace = result.scalars().first()
        
        if not trace:
            logger.error(f"‚ùå Trace {trace_id} not found")
            return {"status": "error", "detail": "trace_not_found"}
        
        prompt_str = f"Inputs: {json.dumps(trace.inputs)}"
        
        # Parallel Evaluation DAG
        tasks = []
        
        # Add Ensemble Judges to Graph
        for judge in self.judges:
            tasks.append(judge.evaluate(db, trace_id, prompt_str, trace.output))
            
        # Add Hallucination Detector Node
        tasks.append(self.hallucination_detector.evaluate(db, trace_id, trace.output))
        
        # Execute all evaluation nodes in parallel
        eval_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Calculate aggregate overall score if judges succeeded
        # Filter for float results (from judges)
        judge_scores = [r for r in eval_results if isinstance(r, (float, int))]
        if judge_scores:
            avg_judge_score = sum(judge_scores) / len(judge_scores)
            trace.overall_score = avg_judge_score
            db.add(trace)
            await db.commit()
        
        results = {
            "judge_count": len(self.judges),
            "results": eval_results
        }
        
        logger.info(f"üìä Evaluation graph complete for trace {trace_id}")
        return results


# Singleton instance
evaluation_service = EvaluationService()

